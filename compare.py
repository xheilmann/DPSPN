import argparse
import itertools
import os
import time

import numpy as np
import pandas as pd

from numpy.random.mtrand import RandomState
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import roc_auc_score, average_precision_score
from sklearn.naive_bayes import GaussianNB
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import MinMaxScaler
from synthesis.synthesizers.privbayes import PrivBayes

from dpspn.dpspn import learn_dp_classifier, learn_dp_mspn
from dpspn.features import get_context
from mst.dataset import Dataset
from mst.mst import MST
from pate_gan.pate_gan import pategan
from spn.algorithms.Sampling import sample_instances
from spn.structure.StatisticalTypes import MetaType



def transform_pg(scalar, data, context):
    synth_data= scalar.inverse_transform(data)
    for c in range(len(context.meta_types)):
        entry= context.meta_types[c]
        if entry== MetaType.BINARY or entry==MetaType.DISCRETE:
            for row in synth_data:
                row[c]=round(row[c])

    return synth_data

def print_save_results(results,  classification, path):


    if not os.path.isfile(opt.result_data_path):
        raise Exception('Result path does not exist')


    if classification :
        data = pd.DataFrame(results[1:-1],
                            columns=['name', 'AUC-Original',  'AUC-DPSPN-Synthetic', 'AUC-PATEGAN-Synthetic', 'AUC-MST-Synthetic','AUC-PrivBayes-Synthetic','APR-Original','APR-DPSPN-Synthetic', 'APR-PATEGAN-Synthetic', 'APR-MST-Synthetic' 'APR-PrivBayes-Synthetic'])
        print(data)
        data.to_csv(path, index=False, mode="a")
        for row in data.values[1:]:
            print(f"{row[0]}&{round(row[1],2)}&{round(row[2],2)}&{round(row[3],2)}&{round(row[4],2)}&{round(row[5],2)}&{round(row[6],2)}&{round(row[7],2)}&{round(row[8],2)}&{round(row[9],2)}&{round(row[10],2)}\\\\")
    print("Saved results at : ", path)



parser = argparse.ArgumentParser()
parser.add_argument('--target-variable', required=True, help='Required if data has a target class')
parser.add_argument('--train-data-path', required=True)
parser.add_argument('--test-data-path', required=True)
parser.add_argument('--feature-data-path', required=True, help='Path to feature-metatypes and bounds')
parser.add_argument('--result-data-path', required=True, help='.csv to store results')
parser.add_argument('--dpspn-syn-data-path', required=False, help='synthetic data generated by dpspn')
parser.add_argument("--dpspn-slice",type=int, required=True, help="slice of dpspn")
parser.add_argument("--iterations", type=int, default=50, help="Number of times algorithms are performed")
parser.add_argument('--mst-domain', help='mst-domain file in json format to use')
parser.add_argument('--epsilon', type=float, default=1.0, help='Epsilon differential privacy parameter')
parser.add_argument('--delta', type=float, default=0.0, help='Delta differential privacy parameter')
parser.add_argument('--pg-teachers', type=int, default=5, help='Number of teachers for pategan')
parser.add_argument('--rongauss-dimension', type=int, default=4, help='dimension ron-gauss algorithm reduces too')
parser.add_argument('--save-synthetic', action='store_true', help='Save the synthetic data into csv')
parser.add_argument('--output-data-path', help='Required if synthetic data needs to be saved')

opt = parser.parse_args()


e = opt.epsilon
delta = opt.delta
train_df = pd.read_csv(opt.train_data_path)
test_df = pd.read_csv(opt.test_data_path)
data_columns = [col for col in train_df.columns]

test_labels = test_df[opt.target_variable].values.copy()
test_df[opt.target_variable] = [np.nan for _ in range((test_df.shape[0]))]
target_index = train_df.columns.get_loc(opt.target_variable)

train = train_df.values.astype("float")
test = test_df.values.astype("float")

ds_context = get_context(opt.feature_data_path)
results = []
train_class = np.delete(train, target_index, axis=1)
test_class = np.delete(test, target_index, axis=1)



def run_class_alg(samples, test_class):
    learners = []
    auc = []
    auprc = []

    learners.append((LogisticRegression(random_state=RandomState(123))))
    learners.append((RandomForestClassifier(random_state=RandomState(123))))
    learners.append((MLPClassifier(early_stopping=True, random_state=RandomState(123))))
    learners.append((GaussianNB()))
    learners.append((GradientBoostingClassifier(random_state=RandomState(123))))
    syn_y = np.round(samples[:, target_index])
    syn_x = np.delete(samples, target_index, axis=1)

    for i in range(0, len(learners)):
        score = learners[i].fit(syn_x, syn_y)
        pred_probs = learners[i].predict_proba(test_class)[:, 1]
        auc_score = roc_auc_score(test_labels, pred_probs)
        auprc_score = average_precision_score(test_labels, pred_probs)
        auc.append(auc_score)
        auprc.append(auprc_score)
    return auc, auprc
#DPSPN
best_perf=0.0
dp_auc_iter=[]
sample = [np.nan for _ in range(len(train[0]))]
DP_start=time.time()
dpspn_slice=opt.dpspn_slice
for it in range(opt.iterations):
    dp_mspn, max_op_on_dataset_slice = learn_dp_classifier(train, ds_context, learn_dp_mspn, target_index, min_instances_slice=dpspn_slice, epsilon=(e/10), total_operations=9, cols=None)
    samples = np.array(sample * train.shape[0]).reshape(-1, len(sample))
    dp_samples = sample_instances(dp_mspn, samples, RandomState(123))
    syn_y = dp_samples[:, target_index]
    syn_x = np.delete(dp_samples, target_index, axis=1)
    unique, counts = np.unique(syn_y, return_counts=True)


    if len(counts) > 1:
        learner = LogisticRegression(random_state=RandomState(123))
        score = learner.fit(syn_x,syn_y)
        pred_probs = learner.predict_proba(test_class)[:, 1]
        temp_perf = roc_auc_score(test_labels, pred_probs)
        dp_auc_iter.append(temp_perf)
    else:
        dp_auc_iter.append(temp_perf)
        continue


    # Select best synthetic data
    if temp_perf > best_perf:
        best_perf = temp_perf.copy()
        synth_train_data = dp_samples.copy()

    print('Iteration: ' + str(it + 1))
    print('DPSPN-Best-Perf:' + str(best_perf))

DP_end=time.time()
DP_time = DP_end -DP_start


dpspn_auc, dpspn_auprc= run_class_alg(synth_train_data, test_class)


#PATEGAN
parameters = {'n_s': 5, 'batch_size':64, 'k': opt.pg_teachers,
              'epsilon': np.round(e,2), 'delta': delta,
              'lamda': 1.0}

scalar = MinMaxScaler()
train_data = scalar.fit_transform(train)
data_dim = train_data.shape[1]
best_perf = 0.0
pategan_auc_iter=[]
Pate_start=time.time()
for it in range(opt.iterations):

    synth_train_data_temp = transform_pg(scalar, pategan(train_data, parameters), ds_context)
    unique, counts = np.unique(synth_train_data_temp[:, (data_dim - 1)], return_counts=True)
    print(counts)

    if len(counts) > 1:
        learner = LogisticRegression(random_state=RandomState(123))
        score = learner.fit(synth_train_data_temp[:, :(data_dim - 1)], np.round(synth_train_data_temp[:, (data_dim - 1)]))
        pred_probs = learner.predict_proba(test_class)[:, 1]
        temp_perf = roc_auc_score(test_labels, pred_probs)
        pategan_auc_iter.append(temp_perf)
    else:
        pategan_auc_iter.append(temp_perf)
        continue


    # Select best synthetic data
    if temp_perf > best_perf:
        best_perf = temp_perf.copy()
        synth_train_data_pate = synth_train_data_temp.copy()

    print('Iteration: ' + str(it + 1))
    print('PATEGAN-Best-Perf:' + str(best_perf))

Pate_end = time.time()
Pate_time = Pate_end - Pate_start

pate_auc, pate_auprc =run_class_alg(synth_train_data_pate, test_class)


# MST

degree=2
num_marginals=None
max_cells=10000

data = Dataset.load(opt.train_data_path, opt.mst_domain)


workload = list(itertools.combinations(data.domain, degree))
workload = [cl for cl in workload if data.domain.size(cl) <= max_cells]
if num_marginals is not None:
    workload = [workload[i] for i in np.random.choice(len(workload), num_marginals, replace=False)]
best_perf = 0.0
mst_auc_iter = []

mst_start=time.time()
for it in range(opt.iterations):

    synth_train_data_temp = MST(data, np.round(e,2), delta)
    unique, counts= np.unique(synth_train_data_temp.df.values[:, (data_dim - 1)], return_counts=True)
    print(counts)

    if len(counts)>1:
        learner = LogisticRegression(random_state=RandomState(123))
        score = learner.fit(synth_train_data_temp.df.values[:, :(data_dim - 1)], (synth_train_data_temp.df.values[:, (data_dim - 1)]))
        pred_probs = learner.predict_proba(test_class)[:, 1]
        temp_perf = roc_auc_score(test_labels, pred_probs)
        mst_auc_iter.append(temp_perf)
    else:
        mst_auc_iter.append(temp_perf)
        continue


    # Select best synthetic data
    if temp_perf > best_perf:
        best_perf = temp_perf
        synth_train_data_mst = synth_train_data_temp.df

    print('Iteration: ' + str(it + 1))
    print('MST-Best-Perf:' + str(best_perf))

mst_end = time.time()
mst_time = mst_end - mst_start

mst_auc, mst_auprc =run_class_alg(synth_train_data_mst.values, test_class)


#PrivBayes
best_perf = 0.0
privbayes_auc_iter= []
privbayes_start=time.time()
for it in range(opt.iterations):
    pb = PrivBayes(epsilon=e)
    pb.fit(train_df)

    synth_train_data_temp = pb.sample()
    unique, counts= np.unique(synth_train_data_temp.values[:, (data_dim - 1)], return_counts=True)
    print(counts)

    if len(counts)>1:
        learner = LogisticRegression(random_state=RandomState(123))
        score = learner.fit(synth_train_data_temp.values[:, :(data_dim - 1)], (synth_train_data_temp.values[:, (data_dim - 1)]))
        pred_probs = learner.predict_proba(test_class)[:, 1]
        temp_perf = roc_auc_score(test_labels, pred_probs)
        privbayes_auc_iter.append(temp_perf)
    else:
        privbayes_auc_iter.append(temp_perf)
        continue


    # Select best synthetic data
    if temp_perf > best_perf:
        best_perf = temp_perf
        synth_train_data_privbayes = synth_train_data_temp.copy()

    print('Iteration: ' + str(it + 1))
    print('PrivBayes-Best-Perf:' + str(best_perf))

privbayes_end = time.time()
privbayes_time = privbayes_end - privbayes_start
privbayes_auc, privbayes_auprc =run_class_alg(synth_train_data_privbayes.values, test_class)


names = ['Logistic Regression', 'Random Forest', 'Neural Network', 'GaussianNB', 'Gradient Boosting Classifier']

auc, auprc= run_class_alg(train, test_class)

for i in range(len(names)):
    results.append([names[i], auc[i], dpspn_auc[i], pate_auc[i], mst_auc[i], privbayes_auc[i],  auprc[i], dpspn_auprc[i], pate_auprc[i], mst_auprc[i], privbayes_auprc[i]])

results.append(["average", np.mean(auc), np.mean(dpspn_auc), np.mean(pate_auc),np.mean(mst_auc), np.mean(privbayes_auc), np.mean(auprc), np.mean(dpspn_auprc), np.mean(pate_auprc), np.mean(mst_auprc), np.mean(privbayes_auprc)])


if not os.path.isdir(opt.output_data_path):
    raise Exception('Output directory does not exist')

X_syn_df = pd.DataFrame(data=synth_train_data_pate, columns=data_columns, dtype=float)
X_syn_df.to_csv(opt.output_data_path + f"/synthetic_data_pate_{e}.csv", index=False, float_format='%.1f')
X_syn_df = pd.DataFrame(data=synth_train_data_privbayes, columns=data_columns)
X_syn_df.to_csv(opt.output_data_path + f"/synthetic_data_privbayes_{e}.csv", index=False,
                float_format='%.1f')
X_syn_df = pd.DataFrame(data=synth_train_data_mst.values, columns=data_columns, dtype=float)
X_syn_df.to_csv(opt.output_data_path + f"/synthetic_data_mst_{e}.csv", index=False,
                float_format='%.1f')
print("Saved synthetic data at : ", opt.output_data_path)


print_save_results(results, True, opt.result_data_path)
